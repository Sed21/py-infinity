[project]
name = "infinity_emb"
version = "0.1.0"
description = "High-performance CPU text embedding server with ONNX"
readme = "README.md"
license = "MIT"
requires-python = ">=3.13"
authors = [{ name = "michaelfeil", email = "noreply@michaelfeil.eu" }]
keywords = ["vector", "embedding", "neural", "search", "onnx", "cpu"]
classifiers = [
    "Development Status :: 4 - Beta",
    "Intended Audience :: Developers",
    "License :: OSI Approved :: MIT License",
    "Programming Language :: Python :: 3.13",
    "Programming Language :: Python :: 3.14",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
]

# Minimal core - ONNX inference only (~250 MB)
dependencies = [
    "numpy>=1.26.0,<3",
    "huggingface_hub>=0.32.0",
    "onnxruntime>=1.18.0",
    "transformers[sentencepiece]>=4.47.0",
]

[project.optional-dependencies]
# Server dependencies
server = [
    "fastapi>=0.103.2",
    "orjson>=3.9.8,!=3.10.0",
    "prometheus-fastapi-instrumentator>=6.1.0",
    "pydantic>=2.4.0,<3",
    "rich>=13",
    "typer>=0.12.5",
    "uvicorn[standard]>=0.32.0",
]
# Optimum for ONNX export/inference (pulls torch ~700MB)
optimum = ["optimum[onnxruntime]>=2.0.0"]
# torch backend for maximum compatibility (~700MB)
torch = ["torch>=2.2.1", "sentence-transformers>=3.2.0"]
# Utilities
logging = ["rich>=13"]
cache = ["diskcache"]
# Preset configurations
slim = ["infinity_emb[server,cache,logging]"]               # ~300 MB
full = ["infinity_emb[server,cache,logging,torch,optimum]"] # ~1.1 GB

[project.scripts]
infinity_emb = "infinity_emb.cli:cli"

[project.urls]
Homepage = "https://github.com/Sed21/py-infinity"
Repository = "https://github.com/Sed21/py-infinity"

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["infinity_emb"]

[tool.uv]
extra-index-url = ["https://download.pytorch.org/whl/cpu"]

[dependency-groups]
test = [
    "pytest>=8.0.0",
    "pytest-mock",
    "httpx",
    "asgi-lifespan",
    "anyio",
    "trio",
    "coverage[toml]>=7.3.2",
    "mypy>=1.12.0",
    "requests>=2.32.0",
    "types-requests>=2.28.0",
    "openai",
]
lint = ["ruff>=0.7.0", "types-toml>=0.10.8.1"]

[tool.pytest.ini_options]
markers = [
    "performance: tests that measure performance (deselect with '-m \"not performance\"')",
]

[tool.ruff]
line-length = 100
